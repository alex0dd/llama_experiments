{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432f8e64-1fc5-4349-865c-8f4e6345023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexo/miniconda3/envs/llama_exploration/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from parsers import ModelParser\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_parser = ModelParser([\n",
    "    \"./Meta-Llama-3-8B/model-00001-of-00004.safetensors\",\n",
    "    \"./Meta-Llama-3-8B/model-00002-of-00004.safetensors\",\n",
    "    \"./Meta-Llama-3-8B/model-00003-of-00004.safetensors\",\n",
    "    \"./Meta-Llama-3-8B/model-00004-of-00004.safetensors\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca78260-ec0f-4339-909f-20a9651e1d9e",
   "metadata": {},
   "source": [
    "TODO: support bs>1 text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4793-b1c3-4e06-987a-e976bd1bfc82",
   "metadata": {},
   "source": [
    "## Prepare text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76260c3a-f3fd-401d-9d11-a342a696d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd5102a-1958-46e7-b4f1-b8a1d7e17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Meta-Llama-3-8B/\")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb7852c-02a4-4899-8828-eef0fe0dd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(text, tokenizer):\n",
    "    if type(text) != list: text = [text]\n",
    "    input_ids = tokenizer(\n",
    "        text,\n",
    "        #return_tensors=\"pt\",\n",
    "        #padding=True\n",
    "    )[\"input_ids\"]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d4133-d19f-4c83-88f3-688b4b888968",
   "metadata": {},
   "source": [
    "## Forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf0e059-483b-4924-9372-b539a4f2a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": False,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": 128001,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 4096,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 14336,\n",
    "  #\"max_position_embeddings\": 8192,\n",
    "  \"max_position_embeddings\": 2048,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 32,\n",
    "  \"num_hidden_layers\": 32,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_scaling\": None,\n",
    "  \"rope_theta\": 500000.0,\n",
    "  \"tie_word_embeddings\": False,\n",
    "  \"torch_dtype\": torch.bfloat16,\n",
    "  \"transformers_version\": \"4.40.0.dev0\",\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 128256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1ffc14-2f83-49a6-98df-a2cd5c7ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.transformer_ops import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8846d446-76dd-4f44-a446-7b3bedb07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"LLAMA3-8B-PKL-int8\"\n",
    "model = Transformer(model_dir, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199e1a58-30b0-490c-9319-10601e0cdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Taken from: https://github.com/meta-llama/llama3/blob/main/llama/generation.py\n",
    "    \n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, tokenizer, input_ids, max_gen_len, temperature=0.6, top_p=0.9, stop_tokens_ids=None, streaming=False, echo=False):\n",
    "    \"\"\"\n",
    "    If temperature > 0, then top_p is used for sampling.\n",
    "\n",
    "    echo: whether to output also input tokens or not.\n",
    "    \"\"\"\n",
    "    device=model.device\n",
    "    max_seq_len = model.max_seq_len\n",
    "    min_prompt_len = min(len(t) for t in input_ids)\n",
    "    max_prompt_len = max(len(t) for t in input_ids)\n",
    "    assert max_prompt_len <= max_seq_len\n",
    "    total_len = min(max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "    pad_id = tokenizer.eos_token_id\n",
    "    batch_size = len(input_ids)\n",
    "    prev_pos = 0\n",
    "    \n",
    "    tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "    for k, t in enumerate(input_ids):\n",
    "        tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "    \n",
    "    eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "    input_text_mask = tokens != pad_id\n",
    "\n",
    "    if stop_tokens_ids == None:\n",
    "        stop_tokens = torch.tensor([13], device=\"cpu\") # 13\n",
    "        #stop_tokens = torch.tensor(list(tokenizer.stop_tokens))\n",
    "    else:\n",
    "        stop_tokens = torch.tensor(stop_tokens_ids, device=\"cpu\")\n",
    "\n",
    "    tokens_output = []\n",
    "\n",
    "    for cur_pos in range(min_prompt_len, total_len):\n",
    "        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        decoded_token = tokenizer.decode(next_token)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        \"\"\"\n",
    "        Needs to be on CPU:\n",
    "        NotImplementedError: The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
    "        \"\"\"\n",
    "        is_in = torch.isin(next_token.cpu(), stop_tokens).to(device)\n",
    "        eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "            is_in\n",
    "        )\n",
    "        prev_pos = cur_pos\n",
    "        if all(eos_reached):\n",
    "            break\n",
    "\n",
    "    tokens_output = []\n",
    "    for idx, generated_tokens in enumerate(tokens.tolist()):\n",
    "        current_prompt_len = len(input_ids[idx])\n",
    "        start_pos = 0 if echo else current_prompt_len\n",
    "        generated_tokens = generated_tokens[start_pos: current_prompt_len + max_gen_len]\n",
    "        for stop_token in stop_tokens_ids:\n",
    "            try:\n",
    "                idx_of_stop_token = generated_tokens.index(stop_token)\n",
    "                generated_tokens = generated_tokens[:idx_of_stop_token]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        tokens_output.append(generated_tokens)\n",
    "    \n",
    "    return [tokenizer.decode(generated_tokens) for generated_tokens in tokens_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e1811d-7035-4250-9032-c1df3e6a4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493db226-caa2-48a1-a778-1b1ebf3cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"People in Italy\", \"The secret of a good cheesecake is\", \"I understand what C++ is\"]\n",
    "input_ids = text_to_ids(texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ccc8d-8118-41bf-9ec2-bccc693317bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacff3d7-d1de-49ed-975c-c371bd67f791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363f8e3b-f2bb-47ef-b8bb-cd58df47d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People in Italy are still reeling from the worst earthquake to hit the country in decades. The quake, which struck the country’s central region on August 24, has left more than 280 people dead and thousands of others injured.\\nThe quake, which registered 6.2 on the Richter scale, destroyed many buildings and infrastructure, and the death toll is expected to rise.\\nThe earthquake has also had a devastating impact on the country’s economy. Tourism, which is a major source of revenue for the country, has been hit hard. Many tourists have cancelled their trips to Italy, and those who are still visiting are staying away from the affected areas', 'The secret of a good cheesecake is in the crust. It has to be crisp, crumbly, and buttery. The filling should be light and fluffy. The toppings should be fresh and flavorful. A good cheesecake is a thing of beauty.\\nIt’s a treat that can be enjoyed any time of year, but it’s especially popular during the holidays. Cheesecake is a dessert that is enjoyed by people of all ages. It’s a treat that can be enjoyed with a cup of coffee or a glass of milk.\\nCheesecake is a dessert that is enjoyed by people of all ages. It’s a treat that can be enjoyed with a cup of coffee', 'I understand what C++ is, and I understand what C is. I understand what the difference between the two is, and I understand what C++ was made for. I know that C++ is a superset of C, and that C is a subset of C++. I know that C++ is a superset of C, and that C is a subset of C++.\\nI also know that C++ is a superset of C, and that C is a subset of C++. I also know that C++ is a superset of C, and that C is a subset of C++.\\nI know that C++ is a superset of C,']\n",
      "Generation took 120.0381920337677 seconds, 0.07507078926439506 tokens/s.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputs = generate_text(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators)\n",
    "delta_time = time.time() - start_time\n",
    "print([text+output for text, output in zip(texts, outputs)])\n",
    "outputs_total_tokens = sum([len(output) for output in outputs])\n",
    "print(f\"Generation took {delta_time} seconds, {delta_time/outputs_total_tokens} tokens/s.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
