{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432f8e64-1fc5-4349-865c-8f4e6345023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexo/miniconda3/envs/llama_exploration/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from utils.utils import load_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4793-b1c3-4e06-987a-e976bd1bfc82",
   "metadata": {},
   "source": [
    "## Prepare text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76260c3a-f3fd-401d-9d11-a342a696d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "base_model_dir = \"./granite-3b-code-instruct/\"\n",
    "config = load_json(f\"./{base_model_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd5102a-1958-46e7-b4f1-b8a1d7e17d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, clean_up_tokenization_spaces=False)\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb7852c-02a4-4899-8828-eef0fe0dd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(text, tokenizer):\n",
    "    if type(text) != list: text = [text]\n",
    "    input_ids = tokenizer(\n",
    "        text,\n",
    "    )[\"input_ids\"]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d4133-d19f-4c83-88f3-688b4b888968",
   "metadata": {},
   "source": [
    "## Forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1ffc14-2f83-49a6-98df-a2cd5c7ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.transformer_ops import Transformer\n",
    "from ops.generation import generate_text, generate_text_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2def2fa7-c559-45ba-84f3-c94a3e381a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_type\"] = \"granite-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba36694-1b70-4f0f-b600-11b8c04beabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['LlamaForCausalLM'],\n",
       " 'attention_bias': True,\n",
       " 'attention_dropout': 0.1,\n",
       " 'bos_token_id': 0,\n",
       " 'eos_token_id': 0,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 2560,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 10240,\n",
       " 'max_position_embeddings': 2048,\n",
       " 'mlp_bias': True,\n",
       " 'model_type': 'granite-small',\n",
       " 'num_attention_heads': 32,\n",
       " 'num_hidden_layers': 32,\n",
       " 'num_key_value_heads': 32,\n",
       " 'pad_token_id': 0,\n",
       " 'pretraining_tp': 1,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_scaling': None,\n",
       " 'rope_theta': 10000,\n",
       " 'tie_word_embeddings': True,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.41.0.dev0',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 49152}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8846d446-76dd-4f44-a446-7b3bedb07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"GRANITE-3B-CODE-INSTRUCT-PKL\"\n",
    "model = Transformer(model_dir, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "493db226-caa2-48a1-a778-1b1ebf3cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = [\"def quicksort():\\t#Implementation of quicksort:\"]\n",
    "#input_ids = text_to_ids(texts, tokenizer)\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Write a code to find the maximum value in a list of numbers.\" },\n",
    "]\n",
    "chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "input_ids = [tokenizer(chat)[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e88e41-bebd-4450-866b-4e73d3b89eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question:\\nWrite a code to find the maximum value in a list of numbers.\\n\\nAnswer:\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6054767e-5d5d-4765-ad3e-3f0fc06db51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363f8e3b-f2bb-47ef-b8bb-cd58df47d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def find_max_value(nums):\n",
      "    max_value = nums[0]\n",
      "    for num in nums:\n",
      "        if num > max_value:\n",
      "            max_value = num\n",
      "    return max_value\n",
      "\n",
      "nums = [2, 3, 5, 7, 11]\n",
      "max_value = find_max_value(nums)\n",
      "print(\"The maximum value in the list is:\", max_value)\n",
      "```\n",
      "Generation took 10.620662689208984 seconds, 9.038983988968958 tokens/s.\n"
     ]
    }
   ],
   "source": [
    "if streaming:\n",
    "    output_text = []\n",
    "    total_tokens_count = 0\n",
    "    start_time = time.time()\n",
    "    for word, n_tokens in generate_text_stream(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators):\n",
    "        print(f\"{word}\", end='', flush=True)\n",
    "        output_text.append(word)\n",
    "        total_tokens_count += n_tokens\n",
    "    delta_time = time.time() - start_time\n",
    "    output_text = \"\".join(output_text)\n",
    "    print()\n",
    "else:  \n",
    "    start_time = time.time()\n",
    "    outputs, total_tokens_count = generate_text(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators)\n",
    "    delta_time = time.time() - start_time\n",
    "    print([text+output for text, output in zip(texts, outputs)])\n",
    "print(f\"Generation took {delta_time} seconds, {total_tokens_count/delta_time} tokens/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b05b0-3054-4023-8b75-97b4ab7f52a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
