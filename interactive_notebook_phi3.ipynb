{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432f8e64-1fc5-4349-865c-8f4e6345023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from utils.utils import load_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4793-b1c3-4e06-987a-e976bd1bfc82",
   "metadata": {},
   "source": [
    "## Prepare text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76260c3a-f3fd-401d-9d11-a342a696d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "base_model_dir = \"./Phi-3-mini-4k-instruct\"\n",
    "config = load_json(f\"./{base_model_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd5102a-1958-46e7-b4f1-b8a1d7e17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, clean_up_tokenization_spaces=False)\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb7852c-02a4-4899-8828-eef0fe0dd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(text, tokenizer):\n",
    "    if type(text) != list: text = [text]\n",
    "    input_ids = tokenizer(\n",
    "        text,\n",
    "    )[\"input_ids\"]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d4133-d19f-4c83-88f3-688b4b888968",
   "metadata": {},
   "source": [
    "## Forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1ffc14-2f83-49a6-98df-a2cd5c7ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.transformer_ops import Transformer\n",
    "from ops.generation import generate_text, generate_text_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8846d446-76dd-4f44-a446-7b3bedb07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"PHI3-MINI-4K-PKL-int8\"\n",
    "model = Transformer(model_dir, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493db226-caa2-48a1-a778-1b1ebf3cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"<|user|>I am going to Paris, what should I see?<|end|><|assistant|>\"]\n",
    "input_ids = text_to_ids(texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6054767e-5d5d-4765-ad3e-3f0fc06db51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363f8e3b-f2bb-47ef-b8bb-cd58df47d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96]) torch.Size([1, 32, 15, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 16, 96]) torch.Size([1, 32, 16, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n",
      "torch.Size([1, 32, 1, 96]) torch.Size([1, 32, 17, 96]) torch.Size([1, 32, 17, 96])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m total_tokens_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, n_tokens \u001b[38;5;129;01min\u001b[39;00m generate_text_stream(model, tokenizer, input_ids, max_gen_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, stop_tokens_ids\u001b[38;5;241m=\u001b[39mterminators):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     output_text\u001b[38;5;241m.\u001b[39mappend(word)\n",
      "File \u001b[0;32m~/Documents/projects/llama3_exploration_project/ops/generation.py:135\u001b[0m, in \u001b[0;36mgenerate_text_stream\u001b[0;34m(model, tokenizer, input_ids, max_gen_len, temperature, top_p, stop_tokens_ids, stream_interval)\u001b[0m\n\u001b[1;32m    132\u001b[0m to_send_n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[0;32m--> 135\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_exploration/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/llama3_exploration_project/ops/transformer_ops.py:228\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    226\u001b[0m     cache_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaches_memory[layer_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    227\u001b[0m     block_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_weights[layer_idx]\n\u001b[0;32m--> 228\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_transformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_rope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m h \u001b[38;5;241m=\u001b[39m functional_rmsnorm(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_norm_weights)\n\u001b[1;32m    230\u001b[0m output \u001b[38;5;241m=\u001b[39m linear_quantized(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_embedding_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_embedding_scales, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_embedding_orig_shape)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Documents/projects/llama3_exploration_project/ops/transformer_ops.py:146\u001b[0m, in \u001b[0;36mfunctional_transformer_block\u001b[0;34m(x, weights, cache_k, cache_v, start_pos, freqs_rope, config, mask)\u001b[0m\n\u001b[1;32m    144\u001b[0m attended_x \u001b[38;5;241m=\u001b[39m functional_rmsnorm(x, weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_layernorm.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    145\u001b[0m hidden \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m functional_gqa_quantized(attended_x, start_pos, weights, cache_k, cache_v, freqs_rope, n_rep, n_kv_heads, n_heads, head_dim, mask, model_type)\n\u001b[0;32m--> 146\u001b[0m attended_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_rmsnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost_attention_layernorm.weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    148\u001b[0m     output \u001b[38;5;241m=\u001b[39m hidden \u001b[38;5;241m+\u001b[39m functional_ffn_phi3_quantized(attended_hidden, weights)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_exploration/lib/python3.10/site-packages/torch/utils/_contextlib.py:112\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgeneratorfunction(func):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_generator(ctx_factory, func)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if streaming:\n",
    "    output_text = []\n",
    "    total_tokens_count = 0\n",
    "    start_time = time.time()\n",
    "    for word, n_tokens in generate_text_stream(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators):\n",
    "        print(f\"{word}\", end='', flush=True)\n",
    "        output_text.append(word)\n",
    "        total_tokens_count += n_tokens\n",
    "    delta_time = time.time() - start_time\n",
    "    output_text = \"\".join(output_text)\n",
    "    print()\n",
    "else:  \n",
    "    start_time = time.time()\n",
    "    outputs, total_tokens_count = generate_text(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators)\n",
    "    delta_time = time.time() - start_time\n",
    "    print([text+output for text, output in zip(texts, outputs)])\n",
    "print(f\"Generation took {delta_time} seconds, {total_tokens_count/delta_time} tokens/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb2b0e-8ece-46d5-8360-52d74e83e0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
