{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432f8e64-1fc5-4349-865c-8f4e6345023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from utils.utils import load_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4793-b1c3-4e06-987a-e976bd1bfc82",
   "metadata": {},
   "source": [
    "## Prepare text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76260c3a-f3fd-401d-9d11-a342a696d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "base_model_dir = \"./Meta-Llama-3-8B/\"\n",
    "config = load_json(f\"./{base_model_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd5102a-1958-46e7-b4f1-b8a1d7e17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, clean_up_tokenization_spaces=False)\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb7852c-02a4-4899-8828-eef0fe0dd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(text, tokenizer):\n",
    "    if type(text) != list: text = [text]\n",
    "    input_ids = tokenizer(\n",
    "        text,\n",
    "    )[\"input_ids\"]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d4133-d19f-4c83-88f3-688b4b888968",
   "metadata": {},
   "source": [
    "## Forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1ffc14-2f83-49a6-98df-a2cd5c7ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.transformer_ops import Transformer\n",
    "from ops.generation import generate_text, generate_text_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8846d446-76dd-4f44-a446-7b3bedb07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"LLAMA3-8B-PKL-int8\"\n",
    "model = Transformer(model_dir, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493db226-caa2-48a1-a778-1b1ebf3cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"People in Italy\"]\n",
    "input_ids = text_to_ids(texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6054767e-5d5d-4765-ad3e-3f0fc06db51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363f8e3b-f2bb-47ef-b8bb-cd58df47d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are being asked to donate their old clothes to help those affected by the devastating earthquake which has hit the country.\n",
      "A clothing collection has been set up in the town of L’Aquila, where the 6.3-magnitude quake killed more than 150 people.\n",
      "The Red Cross is also appealing for food, water and blankets for survivors, who are being treated in makeshift hospitals.\n",
      "The town’s historic centre is believed to have been almost completely destroyed in the quake.\n",
      "The BBC’s Alan Johnston reports from Rome.\n",
      "  1. The earthquake in Italy has been very devastating. The Italian people are very brave and have been very helpful to\n",
      "Generation took 62.71204876899719 seconds, 2.0410750806674183 tokens/s.\n"
     ]
    }
   ],
   "source": [
    "if streaming:\n",
    "    output_text = []\n",
    "    total_tokens_count = 0\n",
    "    start_time = time.time()\n",
    "    for word, n_tokens in generate_text_stream(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators):\n",
    "        print(f\"{word}\", end='', flush=True)\n",
    "        output_text.append(word)\n",
    "        total_tokens_count += n_tokens\n",
    "    delta_time = time.time() - start_time\n",
    "    output_text = \"\".join(output_text)\n",
    "    print()\n",
    "else:  \n",
    "    start_time = time.time()\n",
    "    outputs, total_tokens_count = generate_text(model, tokenizer, input_ids, max_gen_len=128, stop_tokens_ids=terminators)\n",
    "    delta_time = time.time() - start_time\n",
    "    print([text+output for text, output in zip(texts, outputs)])\n",
    "print(f\"Generation took {delta_time} seconds, {total_tokens_count/delta_time} tokens/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb2b0e-8ece-46d5-8360-52d74e83e0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154a8f-80f5-4e2f-844e-a27a4a6c49fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
