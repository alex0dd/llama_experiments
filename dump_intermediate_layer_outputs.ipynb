{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208eff8d-f294-4e94-8405-7992d61e3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1200cc5f-7582-43f2-9158-8c9a3885c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Define the function to register hooks\n",
    "@torch.no_grad()\n",
    "def get_intermediate_outputs(model, input_data, terminators):\n",
    "    outputs = {}\n",
    "    inputs = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "      def hook(module, input, output):\n",
    "          outputs[name] = output#[0].detach()\n",
    "          inputs[name] = input\n",
    "      return hook\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # only register hook on leaf modules\n",
    "            if \"model.layers.0.\" in name or \"model.layers.1.\" in name:\n",
    "                hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "\n",
    "    # Forward pass\n",
    "    model(input_ids)\n",
    "    \"\"\"\n",
    "    outputs_model = model.generate(\n",
    "        input_data,\n",
    "        max_new_tokens=1,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return outputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c1d7e0-a9bd-4cce-bf2b-f797e651609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:644: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556d5acbc5e642b99ece5e5226a2a5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"../Meta-Llama-3-8B/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49a84bc-5497-462f-9a32-c3861276061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")#.to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd8555f-9786-4a4a-bb69-ef1dea5eeecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate outputs have been saved to intermediate_data_llama3_8b.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get intermediate outputs\n",
    "intermediate_outputs, intermediate_inputs = get_intermediate_outputs(model, input_ids, terminators)\n",
    "\n",
    "# Save to pickle file\n",
    "with open('intermediate_data_llama3_8b.pkl', 'wb') as f:\n",
    "    pickle.dump({\"inputs\": intermediate_inputs, \"outputs\": intermediate_outputs}, f)\n",
    "\n",
    "print(\"Intermediate outputs have been saved to intermediate_data_llama3_8b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109e7b74-2377-45fa-9222-f97aa58d2b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0086, -0.0072,  0.0017,  ...,  0.0080,  0.0008, -0.0046],\n",
       "          [ 0.0019,  0.0092,  0.0042,  ...,  0.0089,  0.0005, -0.0122],\n",
       "          [ 0.0027, -0.0045, -0.0003,  ..., -0.0064, -0.0031,  0.0022],\n",
       "          ...,\n",
       "          [ 0.0089, -0.0010, -0.0006,  ...,  0.0103,  0.0020, -0.0016],\n",
       "          [-0.0089, -0.0030,  0.0049,  ...,  0.0003, -0.0013,  0.0082],\n",
       "          [-0.0031,  0.0015,  0.0018,  ..., -0.0017,  0.0006,  0.0023]]],\n",
       "        dtype=torch.bfloat16),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_inputs['model.layers.0.input_layernorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b8b367-48dc-4a60-bf3c-32e5dc69709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0579, -0.1865,  0.0942,  ...,  0.0859,  0.0042, -0.0166],\n",
       "          [ 0.0142,  0.2637,  0.2617,  ...,  0.1040,  0.0027, -0.0488],\n",
       "          [ 0.0204, -0.1328, -0.0201,  ..., -0.0781, -0.0194,  0.0092],\n",
       "          ...,\n",
       "          [ 0.0640, -0.0273, -0.0337,  ...,  0.1187,  0.0116, -0.0063],\n",
       "          [-0.0457, -0.0596,  0.2100,  ...,  0.0023, -0.0055,  0.0229],\n",
       "          [-0.0287,  0.0540,  0.1436,  ..., -0.0261,  0.0042,  0.0117]]],\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([1, 51, 4096]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_outputs['model.layers.0.input_layernorm'], intermediate_outputs['model.layers.0.input_layernorm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4618c89-f9ae-4b5d-a7a9-a9bfdbe36660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0928d3-0ba2-4666-9843-a4bf800093f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens Embedding(128256, 4096)\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm()\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm()\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm()\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm()\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm()\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm()\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm()\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm()\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm()\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm()\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm()\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm()\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm()\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm()\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm()\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm()\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm()\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm()\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm()\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm()\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm()\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm()\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm()\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm()\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm()\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm()\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm()\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm()\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLU()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm()\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLU()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm()\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLU()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm()\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLU()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm()\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm()\n",
      "model.norm LlamaRMSNorm()\n",
      "lm_head Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) == 0:  # only register hook on leaf modules\n",
    "        if \"model.layers.1\" in name or \"model.layers.0\" in name:\n",
    "        print(name, module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
