{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208eff8d-f294-4e94-8405-7992d61e3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1200cc5f-7582-43f2-9158-8c9a3885c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Define the function to register hooks\n",
    "@torch.no_grad()\n",
    "def get_intermediate_outputs(model, input_data, terminators):\n",
    "    outputs = {}\n",
    "    inputs = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "      def hook(module, input, output):\n",
    "          outputs[name] = output#[0].detach()\n",
    "          inputs[name] = input\n",
    "      return hook\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        #if len(list(module.children())) == 0:  # only register hook on leaf modules\n",
    "        if name.startswith(\"model.layers.0.\") or name.startswith(\"model.layers.1.\"):\n",
    "            hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "\n",
    "    # Forward pass\n",
    "    model(input_ids)\n",
    "    \"\"\"\n",
    "    outputs_model = model.generate(\n",
    "        input_data,\n",
    "        max_new_tokens=1,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return outputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c1d7e0-a9bd-4cce-bf2b-f797e651609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:644: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3658a4e04f554e6cbb40b609ec8ff675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"../Meta-Llama-3-8B/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49a84bc-5497-462f-9a32-c3861276061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")#.to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd8555f-9786-4a4a-bb69-ef1dea5eeecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate outputs have been saved to intermediate_data_llama3_8b.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get intermediate outputs\n",
    "intermediate_outputs, intermediate_inputs = get_intermediate_outputs(model, input_ids, terminators)\n",
    "\n",
    "# Save to pickle file\n",
    "with open('intermediate_data_llama3_8b.pkl', 'wb') as f:\n",
    "    pickle.dump({\"inputs\": intermediate_inputs, \"outputs\": intermediate_outputs}, f)\n",
    "\n",
    "print(\"Intermediate outputs have been saved to intermediate_data_llama3_8b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109e7b74-2377-45fa-9222-f97aa58d2b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0086, -0.0072,  0.0017,  ...,  0.0080,  0.0008, -0.0046],\n",
       "          [ 0.0019,  0.0092,  0.0042,  ...,  0.0089,  0.0005, -0.0122],\n",
       "          [ 0.0027, -0.0045, -0.0003,  ..., -0.0064, -0.0031,  0.0022],\n",
       "          ...,\n",
       "          [ 0.0089, -0.0010, -0.0006,  ...,  0.0103,  0.0020, -0.0016],\n",
       "          [-0.0089, -0.0030,  0.0049,  ...,  0.0003, -0.0013,  0.0082],\n",
       "          [-0.0031,  0.0015,  0.0018,  ..., -0.0017,  0.0006,  0.0023]]],\n",
       "        dtype=torch.bfloat16),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_inputs['model.layers.0.input_layernorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b8b367-48dc-4a60-bf3c-32e5dc69709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0579, -0.1865,  0.0942,  ...,  0.0859,  0.0042, -0.0166],\n",
       "          [ 0.0142,  0.2637,  0.2617,  ...,  0.1040,  0.0027, -0.0488],\n",
       "          [ 0.0204, -0.1328, -0.0201,  ..., -0.0781, -0.0194,  0.0092],\n",
       "          ...,\n",
       "          [ 0.0640, -0.0273, -0.0337,  ...,  0.1187,  0.0116, -0.0063],\n",
       "          [-0.0457, -0.0596,  0.2100,  ...,  0.0023, -0.0055,  0.0229],\n",
       "          [-0.0287,  0.0540,  0.1436,  ..., -0.0261,  0.0042,  0.0117]]],\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([1, 51, 4096]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_outputs['model.layers.0.input_layernorm'], intermediate_outputs['model.layers.0.input_layernorm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4618c89-f9ae-4b5d-a7a9-a9bfdbe36660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f0928d3-0ba2-4666-9843-a4bf800093f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm()\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.1.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm()\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm()\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    #if len(list(module.children())) == 0:  # only register hook on leaf modules\n",
    "    if name.startswith(\"model.layers.0.\") or name.startswith(\"model.layers.1.\"):\n",
    "        print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80db81f-2624-451e-b39a-9bb40248cc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
