{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876be1d4-2859-49fd-8776-ff7695c2ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fff082e-e13d-4ffd-9c5c-b4097339b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexo/projects/llama3/experiments/llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../llama3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e7f56f-98d7-4ec1-95ce-ed943bd10ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91fad41-b211-4dc7-95a2-51aaa19b8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:748: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:446.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 8.93 seconds\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir: str = \"../Meta-Llama-3-8B/original/\"\n",
    "tokenizer_path: str = \"../Meta-Llama-3-8B/original/tokenizer.model\"\n",
    "temperature: float = 0.6\n",
    "top_p: float = 0.9\n",
    "max_seq_len: int = 128\n",
    "max_gen_len: int = 64\n",
    "max_batch_size: int = 4\n",
    "\"\"\"\n",
    "Examples to run with the pre-trained models (no fine-tuning). Prompts are\n",
    "usually in the form of an incomplete text prefix that the model can then try to complete.\n",
    "\n",
    "The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n",
    "`max_gen_len` is needed because pre-trained models usually do not stop completions naturally.\n",
    "\"\"\"\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e39740-f029-4845-bc7f-097f9d8bf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts: List[str] = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"I believe the meaning of life is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897cb623-254e-4a13-b88d-f607430a1ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): VocabParallelEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear()\n",
       "        (wk): ColumnParallelLinear()\n",
       "        (wv): ColumnParallelLinear()\n",
       "        (wo): RowParallelLinear()\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear()\n",
       "        (w2): RowParallelLinear()\n",
       "        (w3): ColumnParallelLinear()\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5d1265-447d-4832-9406-7eca56d2a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to find your gift\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = generator.text_completion(\n",
    "    prompts,\n",
    "    max_gen_len=4,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    ")\n",
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77e428d6-7226-4b63-bb17-5298cef807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Define the function to register hooks\n",
    "@torch.no_grad()\n",
    "def get_intermediate_outputs(model, input_ids):\n",
    "    outputs = {}\n",
    "    inputs = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "      def hook(module, input, output):\n",
    "          outputs[name] = output#[0].detach()\n",
    "          inputs[name] = input\n",
    "      return hook\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        #if len(list(module.children())) == 0:  # only register hook on leaf modules\n",
    "        if name.startswith(\"layers.0.\") or name.startswith(\"layers.1.\"):\n",
    "            hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "\n",
    "    # Forward pass\n",
    "    model(input_ids, 0)\n",
    "    \"\"\"\n",
    "    outputs_model = model.generate(\n",
    "        input_data,\n",
    "        max_new_tokens=1,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return outputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3134fcf-1e82-4f51-9446-362dacaea06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = [generator.tokenizer.encode(x, bos=True, eos=False) for x in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a836efc-e196-4801-bcfd-3c065601e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[128000, 40, 4510, 279, 7438, 315, 2324, 374]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee28069c-2b41-42fd-a6f9-d4ddfae59ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "params = generator.model.params\n",
    "bsz = len(prompt_tokens)\n",
    "assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "assert max_prompt_len <= params.max_seq_len\n",
    "total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "pad_id = generator.tokenizer.pad_id\n",
    "tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "\n",
    "prev_pos = 0\n",
    "eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "input_text_mask = tokens != pad_id\n",
    "#if min_prompt_len == total_len:\n",
    "#    logits = self.model.forward(tokens, prev_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b8e379-e82e-4de1-b73e-795fe711569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc7de19-2e87-4e20-ac66-738c278c961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     40,   4510,    279,   7438,    315,   2324,    374,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
       "             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21539a97-3549-4f1d-8bfd-406bfdf95cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate outputs have been saved to intermediate_data_meta_llama3_8b.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get intermediate outputs\n",
    "intermediate_outputs, intermediate_inputs = get_intermediate_outputs(generator.model, tokens)\n",
    "\n",
    "# Save to pickle file\n",
    "with open('intermediate_data_meta_llama3_8b.pkl', 'wb') as f:\n",
    "    pickle.dump({\"inputs\": intermediate_inputs, \"outputs\": intermediate_outputs}, f)\n",
    "\n",
    "print(\"Intermediate outputs have been saved to intermediate_data_meta_llama3_8b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5af11825-717c-4f06-ab7f-6f1d0a8560b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.attention\n",
      "layers.0.attention.wq\n",
      "layers.0.attention.wk\n",
      "layers.0.attention.wv\n",
      "layers.0.attention.wo\n",
      "layers.0.feed_forward\n",
      "layers.0.feed_forward.w1\n",
      "layers.0.feed_forward.w2\n",
      "layers.0.feed_forward.w3\n",
      "layers.0.attention_norm\n",
      "layers.0.ffn_norm\n",
      "layers.1.attention\n",
      "layers.1.attention.wq\n",
      "layers.1.attention.wk\n",
      "layers.1.attention.wv\n",
      "layers.1.attention.wo\n",
      "layers.1.feed_forward\n",
      "layers.1.feed_forward.w1\n",
      "layers.1.feed_forward.w2\n",
      "layers.1.feed_forward.w3\n",
      "layers.1.attention_norm\n",
      "layers.1.ffn_norm\n"
     ]
    }
   ],
   "source": [
    "for name, module in generator.model.named_modules():\n",
    "    if name.startswith(\"layers.0.\") or name.startswith(\"layers.1.\"):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "982453ef-00c1-4939-8235-bace6db299c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7618e-03, -2.9053e-02, -3.1586e-03,  ...,  7.3547e-03,\n",
       "         -4.6875e-02, -2.1606e-02],\n",
       "        [ 2.6367e-02,  3.3264e-03, -8.4839e-03,  ..., -7.5378e-03,\n",
       "         -5.7678e-03,  5.6458e-03],\n",
       "        [-1.2512e-02, -6.9824e-02, -3.8605e-03,  ..., -1.2573e-02,\n",
       "         -4.9805e-02,  2.0508e-02],\n",
       "        ...,\n",
       "        [-5.2795e-03, -1.4709e-02,  4.1504e-02,  ...,  5.4321e-03,\n",
       "         -3.2349e-03,  4.4346e-05],\n",
       "        [ 4.4632e-04,  3.1250e-02, -6.1523e-02,  ..., -2.3804e-03,\n",
       "          1.1444e-03, -1.8768e-03],\n",
       "        [-4.1504e-03, -1.6724e-02,  3.0396e-02,  ...,  8.6060e-03,\n",
       "          8.0872e-04,  3.1433e-03]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()[\"layers.0.attention.wq.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d0401f9-9f95-4566-a3bf-5f1214769421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1040, -0.1543,  0.0737,  ...,  0.0312, -0.0231,  0.0442],\n",
       "        [-0.0447, -0.0293,  0.0396,  ...,  0.0067,  0.0242, -0.0035],\n",
       "        [-0.0564, -0.0869,  0.0188,  ...,  0.0193, -0.0073,  0.0293],\n",
       "        ...,\n",
       "        [ 0.0136,  0.0356, -0.0162,  ..., -0.0177,  0.0018,  0.0102],\n",
       "        [-0.0052, -0.0284,  0.0289,  ...,  0.0135,  0.0055, -0.0042],\n",
       "        [ 0.0039, -0.0100,  0.0118,  ..., -0.0153,  0.0016, -0.0206]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()[\"layers.0.attention.wk.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2632499c-83d1-4389-8d2e-5594de5257ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0089, -0.0020, -0.0005,  ...,  0.0026,  0.0008,  0.0031],\n",
       "        [ 0.0002, -0.0040, -0.0001,  ..., -0.0029, -0.0040,  0.0025],\n",
       "        [ 0.0102,  0.0008,  0.0015,  ..., -0.0062,  0.0080,  0.0070],\n",
       "        ...,\n",
       "        [ 0.0079,  0.0008,  0.0029,  ..., -0.0014, -0.0064, -0.0064],\n",
       "        [ 0.0032,  0.0012,  0.0025,  ...,  0.0027, -0.0046, -0.0011],\n",
       "        [-0.0024, -0.0070,  0.0017,  ...,  0.0033,  0.0071, -0.0034]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()[\"layers.0.attention.wv.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c090de4-f732-4df6-9e27-3240fce7b9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3733e-03,  5.0964e-03, -3.0365e-03,  ...,  2.2888e-03,\n",
       "         -1.9531e-03, -1.7166e-05],\n",
       "        [-2.7313e-03,  1.9379e-03, -1.3733e-03,  ..., -5.1498e-05,\n",
       "         -1.3962e-03, -1.9836e-03],\n",
       "        [ 9.5367e-04, -1.3367e-02,  4.1771e-04,  ...,  2.5940e-03,\n",
       "          7.0496e-03,  4.1809e-03],\n",
       "        ...,\n",
       "        [ 1.8715e-23,  3.2699e-24,  1.8198e-23,  ...,  5.3767e-23,\n",
       "         -2.2360e-24, -1.9852e-23],\n",
       "        [ 1.9335e-23, -1.8612e-24, -1.8818e-23,  ...,  2.3368e-23,\n",
       "          7.3412e-24, -3.1226e-23],\n",
       "        [-7.4860e-23, -6.3693e-23,  5.5059e-24,  ...,  4.9631e-24,\n",
       "         -5.4594e-23, -2.2877e-24]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()['tok_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d2e421-d2b6-459f-8529-9aaf3dfb5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0121, -0.0051, -0.0036,  ...,  0.0149, -0.0134, -0.0030],\n",
       "        [-0.0067, -0.0267, -0.0032,  ...,  0.0131,  0.0046, -0.0016],\n",
       "        [ 0.0110, -0.0005,  0.0135,  ..., -0.0006,  0.0047,  0.0050],\n",
       "        ...,\n",
       "        [-0.0008, -0.0114, -0.0102,  ..., -0.0117,  0.0050, -0.0177],\n",
       "        [-0.0045, -0.0008, -0.0041,  ..., -0.0183, -0.0143,  0.0048],\n",
       "        [-0.0057,  0.0095,  0.0055,  ..., -0.0063,  0.0157, -0.0043]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()['layers.0.feed_forward.w1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5b3ca0-a173-4535-8d9e-91c0311b36c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0087, -0.0151, -0.0090,  ...,  0.0079, -0.0039,  0.0134],\n",
       "        [ 0.0204, -0.0107, -0.0057,  ...,  0.0010,  0.0172,  0.0011],\n",
       "        [ 0.0082, -0.0075, -0.0023,  ..., -0.0018,  0.0025, -0.0165],\n",
       "        ...,\n",
       "        [ 0.0085, -0.0208,  0.0217,  ..., -0.0199,  0.0081, -0.0129],\n",
       "        [-0.0135, -0.0059, -0.0110,  ...,  0.0093,  0.0015, -0.0131],\n",
       "        [-0.0029,  0.0069,  0.0085,  ..., -0.0082, -0.0051, -0.0120]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()['layers.0.feed_forward.w2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24adbbbe-1242-4f2e-908c-6259675dfbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "num_shards = 1\n",
    "params = read_json(\"../Meta-Llama-3-8B/original/params.json\")\n",
    "params = params.get(\"model\", params)\n",
    "n_layers = params[\"n_layers\"]\n",
    "n_heads = params[\"n_heads\"]\n",
    "n_heads_per_shard = n_heads // num_shards\n",
    "dim = params[\"dim\"]\n",
    "dims_per_head = dim // n_heads\n",
    "\n",
    "if params.get(\"n_kv_heads\", None) is not None:\n",
    "    num_key_value_heads = params[\"n_kv_heads\"]  # for GQA / MQA\n",
    "    num_local_key_value_heads = n_heads_per_shard // num_key_value_heads\n",
    "    key_value_dim = dim // num_key_value_heads\n",
    "else:  # compatibility with other checkpoints\n",
    "    num_key_value_heads = n_heads\n",
    "    num_local_key_value_heads = n_heads_per_shard\n",
    "    key_value_dim = dim\n",
    "\n",
    "def permute(w, n_heads, dim1=dim, dim2=dim):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4089d51-c99e-4626-a2bf-5c20d5b0bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_undo_permute(w, n_heads, dim1=dim, dim2=dim):\n",
    "    return w.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc52292-41d1-4e56-8d69-5bb95a41df1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cd2625-0d3b-496d-91fa-e87df8c21d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b985fc-7c88-48d1-b8a0-19f20c848975",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_weight = generator.model.state_dict()[\"layers.0.attention.wq.weight\"].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc90183c-48c8-41f5-849b-44621a5838bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96cd8943-b4fa-4ece-ad1c-9b612b295d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 2, 4096])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq_weight.view(n_heads, dim // n_heads // 2, 2, dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e364a2a3-6c6f-4eb5-9dc0-505abf70a6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 64, 4096])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq_weight.view(n_heads, dim // n_heads // 2, 2, dim).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf7a2a-cc3a-42a8-803f-defef3960f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"model.layers.{layer_i}.self_attn.q_proj.weight\": permute(\n",
    "    loaded[f\"layers.{layer_i}.attention.wq.weight\"], n_heads=n_heads\n",
    "),\n",
    "f\"model.layers.{layer_i}.self_attn.k_proj.weight\": permute(\n",
    "    loaded[f\"layers.{layer_i}.attention.wk.weight\"],\n",
    "    n_heads=num_key_value_heads,\n",
    "    dim1=dim // num_local_key_value_heads,\n",
    "),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4754c651-63a3-445d-b850-2d7f45c3fc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()[\"layers.0.attention.wk.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3146f533-a357-4d55-a11d-41d736d5e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 2, 4096])\n",
      "torch.Size([32, 2, 16, 4096])\n"
     ]
    }
   ],
   "source": [
    "dim1=dim // num_local_key_value_heads\n",
    "dim2=dim\n",
    "print(generator.model.state_dict()[\"layers.0.attention.wk.weight\"].view(n_heads, dim1 // n_heads // 2, 2, dim2).shape)\n",
    "print(generator.model.state_dict()[\"layers.0.attention.wk.weight\"].view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37c3c0a0-82ef-4683-a088-32c94588d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads_per_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "948aa1fd-7a24-43d7-ac6e-63cf5112dbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd3dce0b-8c49-43d1-9cc8-e69e325c08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7618e-03, -2.9053e-02, -3.1586e-03,  ...,  7.3547e-03,\n",
       "         -4.6875e-02, -2.1606e-02],\n",
       "        [ 2.6367e-02,  3.3264e-03, -8.4839e-03,  ..., -7.5378e-03,\n",
       "         -5.7678e-03,  5.6458e-03],\n",
       "        [-1.2512e-02, -6.9824e-02, -3.8605e-03,  ..., -1.2573e-02,\n",
       "         -4.9805e-02,  2.0508e-02],\n",
       "        ...,\n",
       "        [-5.2795e-03, -1.4709e-02,  4.1504e-02,  ...,  5.4321e-03,\n",
       "         -3.2349e-03,  4.4346e-05],\n",
       "        [ 4.4632e-04,  3.1250e-02, -6.1523e-02,  ..., -2.3804e-03,\n",
       "          1.1444e-03, -1.8768e-03],\n",
       "        [-4.1504e-03, -1.6724e-02,  3.0396e-02,  ...,  8.6060e-03,\n",
       "          8.0872e-04,  3.1433e-03]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.state_dict()[\"layers.0.attention.wq.weight\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
